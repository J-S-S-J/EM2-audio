---
title: "d-prime-before"
format: html
editor: visual
---

## Set-up

```{r}
library(tidyverse)
library(rstatix) # for ANOVA
library(lme4) # for LMM
library(lmerTest)
```

```{r}

forced_choice_path <- "~/Desktop/Python/EM2-audio/Analysis/combined_control_data.csv"

forced_choice_df <- read_delim(forced_choice_path) |>
  group_by(`Participant ID`) |> 
  mutate(trial_i = row_number()) |> ungroup()  |>
  rename(ID = `Participant ID`) |>
  relocate(ID, .after = trial_i) |>
  filter(main.thisN >= 0) |> # remove pratice trial
  filter(rt < 2) # must be quicker than 2 sec

d_prime_path <- "~/Desktop/Python/EM2-audio/Analysis/combined_dprime_after_main_experiment.csv"

d_prime_df <- read_delim(d_prime_path) |>
  group_by(`Participant ID`) |> 
  mutate(trial_i = row_number()) |> ungroup()  |>
  rename(ID = `Participant ID`) |>
  relocate(trial_i, ID, .before = signal_type) |>
  filter(main.thisTrialN >= 0) |> # remove pratice trial
  filter(rt < 2) |> # must be quicker than 2 sec 
  filter(compression_level == 0.4)


```

## Forced choice results

```{r}
participant_acc <- forced_choice_df |>
  group_by(ID, prime_valence) |>
  summarise(mean_acc = mean(accuracy, na.rm = TRUE), .groups = "drop")

# 2. Calculate Group Averages
group_summary <- participant_acc |>
  group_by(prime_valence) |>
  summarise(
    group_mean = mean(mean_acc),
    sd = sd(mean_acc),
    n = n(),
    se = sd / sqrt(n)
  )

print(group_summary)
```

```{r}
forced_choice_df |>
  # 1. Calculate total accuracy per person (ignoring valence this time)
  group_by(ID) |>
  summarise(
    total_acc = mean(accuracy, na.rm = TRUE),
    # Calculate a simple standard error for the error bars
    se = sd(accuracy, na.rm = TRUE) / sqrt(n()) 
  ) |>
  # 2. Reorder ID by Accuracy (lowest to highest)
  mutate(ID = fct_reorder(ID, total_acc)) |>
  
  # 3. Plot
  ggplot(aes(x = ID, y = total_acc)) +
  
  # Chance line
  geom_hline(yintercept = 0.5, color = "red", linetype = "dashed") +
  
  # Error bars (Confidence interval approximation)
  geom_errorbar(aes(ymin = total_acc - se, ymax = total_acc + se), width = 0.2) +
  
  # Points
  geom_point(size = 3, color = "steelblue") +
  
  # Labels
  labs(title = "Individual Visibility Check",
       subtitle = "Participants distinct from the red line (0.5) might be aware of the prime.",
       y = "Mean Accuracy",
       x = "Participant ID") +
  
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate IDs to read them
```

## d-prime after result

### Analyse a accuracy

We simply count how many times the participant was "Correct" (either a **Hit** or a **Correct Rejection**) and divide by the total number of trials.

```{r}

simple_accuracy <- d_prime_df |>
  group_by(ID) |>
  summarise(
    # Check if the outcome was one of the two "Correct" types
    n_correct = sum(trial_outcome %in% c("Hit", "Correct Rejection")),
    n_total   = n(),
    accuracy  = n_correct / n_total
  )

# Preview the data
head(simple_accuracy)
```

```{r}
simple_accuracy |>
  ggplot(aes(x = "Participants", y = accuracy)) +
  
  # 1. The Chance Line (50%)
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
  
  # 2. Boxplot to show the group average and spread
  geom_boxplot(fill = "white", color = "black", width = 0.3, outlier.shape = NA) +
  
  # 3. Individual Points (Jittered)
  geom_jitter(width = 0.1, size = 3, alpha = 0.6, color = "steelblue") +
  
  # 4. Labels
  labs(title = "Detection Task Accuracy",
       subtitle = "Red Line = 50% (Random Guessing). Points above are performing better than chance.",
       y = "Percent Correct (Hits + Correct Rejections)",
       x = "") +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent) + # Show Y axis as 0% - 100%
  theme_minimal()
```

```{r}
# One-sample t-test against 0.5
stat_test <- simple_accuracy |>
  t_test(accuracy ~ 1, mu = 0.5, alternative = "greater")

stat_test
```

**How to read the result:**

-   If **`p` \< 0.05**: The participants were **not** guessing; they could hear the sounds significantly better than chance.

-   If **`p` \> 0.05**: The result is not significantly different from 50/50. The masking was effective (they were guessing).

### Analyse as d-prime

```{r}

# 1. Calculate Signal Detection metrics per participant
d_prime_stats <- d_prime_df |>
  group_by(ID) |>
  summarise(
    # Count outcomes
    hits = sum(trial_outcome == "Hit"),
    fas  = sum(trial_outcome == "False Alarm"),
    n_signal = sum(signal_type == "present"),
    n_noise  = sum(signal_type == "absent"),
    .groups = "drop"
  ) |>
  mutate(
    # Log-linear correction (add 0.5) to handle perfect performance (0 or 1 rates)
    hit_rate = (hits + 0.5) / (n_signal + 1),
    fa_rate  = (fas + 0.5) / (n_noise + 1),
    
    # Calculate d' (Sensitivity): Ability to distinguish Signal from Noise
    # 0 = Guessing, > 0 = Detection
    d_prime = qnorm(hit_rate) - qnorm(fa_rate),
    
    # Calculate c (Bias): Tendency to say "Yes" or "No"
    # 0 = Neutral, Negative = Trigger Happy (Yes!), Positive = Conservative (No!)
    c_bias = -0.5 * (qnorm(hit_rate) + qnorm(fa_rate))
  )

# View the first few rows
head(d_prime_stats)
```

```{r}
# Create a composite plot
library(patchwork) # If you don't have this, you can run the plots separately

# Plot A: Distribution of Sensitivity (d')
p1 <- d_prime_stats|>
  ggplot(aes(x = d_prime)) +
  geom_histogram(binwidth = 0.2, fill = "steelblue", color = "white", alpha = 0.8) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "red", size = 1) +
  labs(title = "Distribution of Sensitivity (d')",
       subtitle = "Red Line = Chance Performance (0).",
       x = "d' (Sensitivity)",
       y = "Count of Participants") +
  theme_minimal()

# Plot B: Bias vs Sensitivity
p2 <- d_prime_stats |>
  ggplot(aes(x = c_bias, y = d_prime, label = ID)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  geom_vline(xintercept = 0, linetype = "dotted", color = "gray") +
  geom_point(size = 3, alpha = 0.6) +
  geom_text(vjust = -0.5, size = 3, check_overlap = TRUE) +
  labs(title = "Participant Strategy Map",
       subtitle = "Are they hearing it (Y-axis) or just guessing (X-axis)?",
       x = "Response Bias (c) \n(< 0 = Liberal/Yes-bias, > 0 = Conservative/No-bias)",
       y = "Sensitivity (d')") +
  theme_minimal()

# Display them
p1
p2
```

```{r}
# Test if d' is significantly greater than 0
test_result <- t.test(d_prime_stats$d_prime, mu = 0, alternative = "greater")

print(test_result)
```

**interpretation:**

-   If **p \< 0.05**, the group **could statistically hear the primes** (the masking wasn't fully effective).

-   If **p \> 0.05**, the group was effectively **deaf** to the primes (ideal for a subliminal experiment)

```{r}
sdt_stats <- d_prime_before |>
  group_by(ID, compression_level) |>
  summarise(
    # Count specific outcomes
    hits = sum(trial_outcome == "Hit"),
    misses = sum(trial_outcome == "Miss"),
    crs = sum(trial_outcome == "Correct Rejection"),
    fas = sum(trial_outcome == "False Alarm"),
    
    # Calculate totals
    n_signal_trials = hits + misses,
    n_noise_trials = crs + fas,
    .groups = "drop"
  ) |>
  
  # 2. Calculate d-prime and Criterion (c) with Log-Linear Correction
  mutate(
    # Add 0.5 to counts and 1 to denominator to handle 0s and 1s (Hautus, 1995)
    hit_rate_adj = (hits + 0.5) / (n_signal_trials + 1),
    fa_rate_adj = (fas + 0.5) / (n_noise_trials + 1),
    
    # Calculate d' (Sensitivity)
    d_prime = qnorm(hit_rate_adj) - qnorm(fa_rate_adj),
    
    # Calculate c (Response Bias)
    # Negative c = Trigger Happy (Says "Present" too much)
    # Positive c = Conservative (Says "Absent" too much)
    c_bias = -0.5 * (qnorm(hit_rate_adj) + qnorm(fa_rate_adj))
  )

# Preview the data
head(sdt_stats)
```
