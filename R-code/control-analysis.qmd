---
title: "d-prime-before"
format: html
editor: visual
---

## Set-up

```{r}
library(tidyverse)
library(rstatix) # for ANOVA
library(lme4) # for LMM
library(lmerTest)

library(ggthemes)
library(extrafont)
```

```{r}

forced_choice_path <- "~/Desktop/Python/EM2-audio/Analysis/combined_control_data.csv"

forced_choice_df <- read_delim(forced_choice_path) |>
  group_by(`Participant ID`) |> 
  mutate(trial_i = row_number()) |> ungroup()  |>
  rename(ID = `Participant ID`) |>
  relocate(ID, .after = trial_i) |>
  filter(main.thisN >= 0) |> # remove pratice trial
  filter(rt < 2) # must be quicker than 2 sec

d_prime_path <- "~/Desktop/Python/EM2-audio/Analysis/combined_dprime_after_main_experiment.csv"

d_prime_df <- read_delim(d_prime_path) |>
  group_by(`Participant ID`) |> 
  mutate(trial_i = row_number()) |> ungroup()  |>
  rename(ID = `Participant ID`) |>
  relocate(trial_i, ID, .before = signal_type) |>
  filter(main.thisTrialN >= 0) |> # remove pratice trial
  filter(rt < 2) |> # must be quicker than 2 sec 
  filter(compression_level == 0.5)


```

## Forced choice results

```{r}
participant_acc <- forced_choice_df |>
  group_by(ID, prime_valence) |>
  summarise(mean_acc = mean(accuracy, na.rm = TRUE), .groups = "drop")

group_summary <- participant_acc |>
  group_by(prime_valence) |>
  summarise(
    group_mean = mean(mean_acc),
    sd = sd(mean_acc),
    n = n(),
    se = sd / sqrt(n)
  )

print(group_summary)
```

```{r}
forced_choice_fig <- forced_choice_df |>
  group_by(ID) |>
  summarise(
    total_acc = mean(accuracy, na.rm = TRUE),
    se = sd(accuracy, na.rm = TRUE) / sqrt(n()) 
  ) |>
  mutate(ID = fct_reorder(ID, total_acc)) |>
  ggplot(aes(x = ID, y = total_acc)) +
  geom_hline(yintercept = 0.5, color = "red", linetype = "dashed", size = 1) +
  geom_errorbar(aes(ymin = total_acc - se, ymax = total_acc + se), width = 0.2) +
  geom_point(size = 3, color = "steelblue") +
  labs(title = "Forced choice accuracy",
       subtitle = "Red line (0.5) indicates random choice",
       y = "Mean Accuracy",
       x = "Participant ID") +
  
  theme_fivethirtyeight() +
  

  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.position = "none", 
    text = element_text(family = "IBM Plex Mono"), 
    plot.title = element_text(size = 12),
    axis.text.x = element_text(angle = 45, hjust = 1, size = 9) 
  ) 


print(forced_choice_fig)


ggsave(
  filename = "forced_choice_fig.png",
  plot = forced_choice_fig,
  width = 8, 
  height = 5,
  units = "in"
)
```

```{r}
stat_test1 <- forced_choice_df |>
  t_test(accuracy ~ 1, mu = 0.5, alternative = "greater")

stat_test1
```

## d-prime after result

### Analyse a accuracy

We simply count how many times the participant was "Correct" (either a **Hit** or a **Correct Rejection**) and divide by the total number of trials.

```{r}

simple_accuracy <- d_prime_df |>
  group_by(ID) |>
  summarise(
    # Check if the outcome was one of the two "Correct" types
    n_correct = sum(trial_outcome %in% c("Hit", "Correct Rejection")),
    n_total   = n(),
    accuracy  = n_correct / n_total
  )

head(simple_accuracy)
```

```{r}
simple_accuracy |>
  ggplot(aes(x = "Participants", y = accuracy)) +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
  geom_boxplot(fill = "white", color = "black", width = 0.3, outlier.shape = NA) +
  geom_jitter(width = 0.1, size = 3, alpha = 0.6, color = "steelblue") +
  labs(title = "Detection Task Accuracy",
       subtitle = "Red Line = 50% (Random Guessing). Points above are performing better than chance.",
       y = "Percent Correct (Hits + Correct Rejections)",
       x = "") +
  scale_y_continuous(limits = c(0, 1), labels = scales::percent) + # Show Y axis as 0% - 100%
  theme_minimal()
```

```{r}
detection_task_fig <- d_prime_df |>
  mutate(ID = as.factor(ID)) |> 
  group_by(ID) |>
  summarise(
    n_trials = n(),
    n_correct = sum(trial_outcome %in% c("Hit", "Correct Rejection")),
    accuracy = n_correct / n_trials,
    # Calculate SE for proportion
    se = sqrt((accuracy * (1 - accuracy)) / n_trials)
  ) |>
  
  # reorder ID by Accuracy (lowest to highest)
  mutate(ID = fct_reorder(ID, accuracy)) |>
  
  ggplot(aes(x = ID, y = accuracy)) +
  
  # Chance line (50%)
  geom_hline(yintercept = 0.5, color = "red", linetype = "dashed", size = 1) +
  
  # Error bars
  geom_errorbar(aes(ymin = accuracy - se, ymax = accuracy + se), width = 0.2) +
  geom_point(size = 3, color = "steelblue") +
  labs(title = "Detection Task Accuracy",
       subtitle = "Red line (0.5) indicates random choice",
       y = "Percent Correct (Hits + Correct Rejections)",
       x = "Participant ID") +
  
  scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +
  theme_fivethirtyeight() +
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    legend.position = "none",
    text = element_text(family = "IBM Plex Mono"),
    plot.title = element_text(size = 12),
    # Rotate x-axis labels so IDs are readable
    axis.text.x = element_text(angle = 45, hjust = 1, size = 9)
  )


print(detection_task_fig)


ggsave(
  filename = "detection_task_fig.png",
  plot = detection_task_fig,
  width = 8, 
  height = 5,
  units = "in"
)
```

```{r}
# One-sample t-test against 0.5
stat_test <- simple_accuracy |>
  t_test(accuracy ~ 1, mu = 0.5, alternative = "greater", )

effect_size <- simple_accuracy |> 
  cohens_d(accuracy ~ 1, mu = 0.5)


stat_test
effect_size
```

**How to read the result:**

-   If **`p` \< 0.05**: The participants were **not** guessing; they could hear the sounds significantly better than chance.

-   If **`p` \> 0.05**: The result is not significantly different from 50/50. The masking was effective (they were guessing).

### 

**interpretation:**

-   If **p \< 0.05**, the group **could statistically hear the primes** (the masking wasn't fully effective).

-   If **p \> 0.05**, the group was effectively **deaf** to the primes (ideal for a subliminal experiment)
