filter(rt < 2) # must be quicker than 2 sec
d_prime_before <- d_prime_df |>
filter(!is.na(compression_level.1))
d_prime_after <- d_prime_df |>
filter(is.na(compression_level.1))
library(tidyverse)
library(rstatix) # for ANOVA
library(lme4) # for LMM
library(lmerTest)
forced_choice_path <- "~/Desktop/Python/EM2-audio/Analysis/combined_control_data.csv"
forced_choice_df <- read_delim(forced_choice_path) |>
group_by(`Participant ID`) |>
mutate(trial_i = row_number()) |> ungroup()  |>
rename(ID = `Participant ID`) |>
relocate(ID, .after = trial_i) |>
filter(main.thisN >= 0) |> # remove pratice trial
filter(rt < 2) # must be quicker than 2 sec
d_prime_path <- "~/Desktop/Python/EM2-audio/Analysis/combined_dprime_after_main_experiment.csv"
d_prime_df <- read_delim(d_prime_path) |>
group_by(`Participant ID`) |>
mutate(trial_i = row_number()) |> ungroup()  |>
rename(ID = `Participant ID`) |>
relocate(trial_i, ID, .before = signal_type) |>
filter(main.thisTrialN >= 0) |> # remove pratice trial
filter(rt < 2) # must be quicker than 2 sec
d_prime_before <- d_prime_df |>
filter(!is.na(compression_level.1))
d_prime_after <- d_prime_df |>
filter(is.na(compression_level.1))
View(d_prime_before)
glimpse(forced_choice_df)
participant_acc <- forced_choice_df |>
group_by(ID, prime_valence) |>
summarise(mean_acc = mean(accuracy, na.rm = TRUE), .groups = "drop")
# 2. Calculate Group Averages
group_summary <- participant_acc |>
group_by(prime_valence) |>
summarise(
group_mean = mean(mean_acc),
sd = sd(mean_acc),
n = n(),
se = sd / sqrt(n)
)
print(group_summary)
participant_acc |>
ggplot(aes(x = prime_valence, y = mean_acc, fill = prime_valence)) +
# 1. Add the chance level line (CRITICAL for forced choice)
geom_hline(yintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
# 2. Add Boxplots to show distribution
geom_boxplot(alpha = 0.6, outlier.shape = NA) +
# 3. Add individual participant points (Jittered)
geom_jitter(width = 0.2, alpha = 0.4) +
# 4. Add the Group Mean (White Diamond)
stat_summary(fun = mean, geom = "point", shape = 23, size = 4, fill = "white") +
labs(title = "Forced Choice Accuracy by Valence",
subtitle = "Red Line = Chance (50%). Points above are 'Seers'.",
y = "Proportion Correct",
x = "Prime Valence") +
theme_minimal() +
scale_fill_brewer(palette = "Set2")
forced_choice_df |>
# 1. Calculate total accuracy per person (ignoring valence this time)
group_by(ID) |>
summarise(
total_acc = mean(accuracy, na.rm = TRUE),
# Calculate a simple standard error for the error bars
se = sd(accuracy, na.rm = TRUE) / sqrt(n())
) |>
# 2. Reorder ID by Accuracy (lowest to highest)
mutate(ID = fct_reorder(ID, total_acc)) |>
# 3. Plot
ggplot(aes(x = ID, y = total_acc)) +
# Chance line
geom_hline(yintercept = 0.5, color = "red", linetype = "dashed") +
# Error bars (Confidence interval approximation)
geom_errorbar(aes(ymin = total_acc - se, ymax = total_acc + se), width = 0.2) +
# Points
geom_point(size = 3, color = "steelblue") +
# Labels
labs(title = "Individual Visibility Check",
subtitle = "Participants distinct from the red line (0.5) might be aware of the prime.",
y = "Mean Accuracy",
x = "Participant ID") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate IDs to read them
glimpse(d_prime_after)
sdt_stats <- d_prime_before |>
group_by(ID, compression_level) |>
summarise(
# Count specific outcomes
hits = sum(trial_outcome == "Hit"),
misses = sum(trial_outcome == "Miss"),
crs = sum(trial_outcome == "Correct Rejection"),
fas = sum(trial_outcome == "False Alarm"),
# Calculate totals
n_signal_trials = hits + misses,
n_noise_trials = crs + fas,
.groups = "drop"
) |>
# 2. Calculate d-prime and Criterion (c) with Log-Linear Correction
mutate(
# Add 0.5 to counts and 1 to denominator to handle 0s and 1s (Hautus, 1995)
hit_rate_adj = (hits + 0.5) / (n_signal_trials + 1),
fa_rate_adj = (fas + 0.5) / (n_noise_trials + 1),
# Calculate d' (Sensitivity)
d_prime = qnorm(hit_rate_adj) - qnorm(fa_rate_adj),
# Calculate c (Response Bias)
# Negative c = Trigger Happy (Says "Present" too much)
# Positive c = Conservative (Says "Absent" too much)
c_bias = -0.5 * (qnorm(hit_rate_adj) + qnorm(fa_rate_adj))
)
# Preview the data
head(sdt_stats)
sdt_stats |>
ggplot(aes(x = factor(compression_level), y = d_prime)) +
# 1. Individual Participant lines (Spaghetti plot)
geom_line(aes(group = ID), color = "gray", alpha = 0.3) +
geom_point(aes(group = ID), color = "gray", alpha = 0.3) +
# 2. Group Average (Thick Red Line)
stat_summary(aes(group = 1), fun = mean, geom = "line", color = "red", size = 1.5) +
stat_summary(aes(group = 1), fun = mean, geom = "point", color = "red", size = 3) +
# 3. Add d' thresholds for context
geom_hline(yintercept = 0, linetype = "dashed") + # Chance
annotate("text", x = 1, y = 0.2, label = "Chance", color = "black", size = 3) +
labs(
title = "Auditory Sensitivity (d') by Compression Level",
subtitle = "Red line = Group Average. Gray lines = Individuals.\nHigher d' means better detection.",
x = "Compression Level (Signal Strength)",
y = "Sensitivity (d')"
) +
theme_minimal()
# Summarize one average value per participant
participant_overall <- sdt_stats |>
group_by(ID) |>
summarise(
avg_d_prime = mean(d_prime),
avg_bias = mean(c_bias)
)
participant_overall |>
ggplot(aes(x = avg_bias, y = avg_d_prime, label = ID)) +
# Quadrant Lines
geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.5) + # No Bias
geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) + # Chance performance
# Points
geom_point(size = 3, color = "steelblue") +
# Add labels for outliers
geom_text(vjust = -0.5, size = 3, check_overlap = TRUE) +
labs(
title = "Participant Performance Map",
subtitle = "Top-Center is ideal (High Sensitivity, No Bias).\nRight = Conservative (Misses signals). Left = Liberal (False Alarms).",
x = "Response Bias (c)",
y = "Average Sensitivity (d')"
) +
theme_minimal()
library(tidyverse)
# 1. Calculate Signal Detection Stats (d') per participant
d_prime_results_after <- d_prime_after |>
group_by(ID) |>
summarise(
hits = sum(trial_outcome == "Hit"),
fas  = sum(trial_outcome == "False Alarm"),
n_signal = sum(signal_type == "present"),
n_noise  = sum(signal_type == "absent")
) |>
mutate(
# Log-linear correction (add 0.5) to avoid infinite values
hit_rate = (hits + 0.5) / (n_signal + 1),
fa_rate  = (fas + 0.5) / (n_noise + 1),
# Calculate d' (Sensitivity)
d_prime = qnorm(hit_rate) - qnorm(fa_rate)
)
# Preview
head(d_prime_results)
library(tidyverse)
# 1. Calculate Signal Detection Stats (d') per participant
d_prime_results_after <- d_prime_after |>
group_by(ID) |>
summarise(
hits = sum(trial_outcome == "Hit"),
fas  = sum(trial_outcome == "False Alarm"),
n_signal = sum(signal_type == "present"),
n_noise  = sum(signal_type == "absent")
) |>
mutate(
# Log-linear correction (add 0.5) to avoid infinite values
hit_rate = (hits + 0.5) / (n_signal + 1),
fa_rate  = (fas + 0.5) / (n_noise + 1),
# Calculate d' (Sensitivity)
d_prime = qnorm(hit_rate) - qnorm(fa_rate)
)
# Preview
head(d_prime_results_after)
View(d_prime_results_after)
View(d_prime_after)
# 2. Calculate Forced Choice Accuracy per participant
fc_results <- forced_choice_df |>
group_by(ID) |>
summarise(
fc_accuracy = mean(accuracy, na.rm = TRUE)
)
# Combine the two datasets
visibility_check <- left_join(d_prime_results, fc_results, by = "ID")
# Combine the two datasets
visibility_check <- left_join(d_prime_results_after, fc_results, by = "ID")
# 1. Calculate Signal Detection metrics per participant
d_prime_stats <- d_prime_after |>
group_by(ID) |>
summarise(
# Count outcomes
hits = sum(trial_outcome == "Hit"),
fas  = sum(trial_outcome == "False Alarm"),
n_signal = sum(signal_type == "present"),
n_noise  = sum(signal_type == "absent"),
.groups = "drop"
) |>
mutate(
# Log-linear correction (add 0.5) to handle perfect performance (0 or 1 rates)
hit_rate = (hits + 0.5) / (n_signal + 1),
fa_rate  = (fas + 0.5) / (n_noise + 1),
# Calculate d' (Sensitivity): Ability to distinguish Signal from Noise
# 0 = Guessing, > 0 = Detection
d_prime = qnorm(hit_rate) - qnorm(fa_rate),
# Calculate c (Bias): Tendency to say "Yes" or "No"
# 0 = Neutral, Negative = Trigger Happy (Yes!), Positive = Conservative (No!)
c_bias = -0.5 * (qnorm(hit_rate) + qnorm(fa_rate))
)
# View the first few rows
head(d_prime_stats)
# Create a composite plot
library(patchwork) # If you don't have this, you can run the plots separately
install.packages('patchwork')
# Create a composite plot
library(patchwork) # If you don't have this, you can run the plots separately
# Plot A: Distribution of Sensitivity (d')
p1 <- d_prime_stats |>
ggplot(aes(x = d_prime)) +
geom_histogram(binwidth = 0.2, fill = "steelblue", color = "white", alpha = 0.8) +
geom_vline(xintercept = 0, linetype = "dashed", color = "red", size = 1) +
labs(title = "Distribution of Sensitivity (d')",
subtitle = "Red Line = Chance Performance (0).",
x = "d' (Sensitivity)",
y = "Count of Participants") +
theme_minimal()
# Plot B: Bias vs Sensitivity
p2 <- d_prime_stats |>
ggplot(aes(x = c_bias, y = d_prime, label = ID)) +
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
geom_vline(xintercept = 0, linetype = "dotted", color = "gray") +
geom_point(size = 3, alpha = 0.6) +
geom_text(vjust = -0.5, size = 3, check_overlap = TRUE) +
labs(title = "Participant Strategy Map",
subtitle = "Are they hearing it (Y-axis) or just guessing (X-axis)?",
x = "Response Bias (c) \n(< 0 = Liberal/Yes-bias, > 0 = Conservative/No-bias)",
y = "Sensitivity (d')") +
theme_minimal()
# Display them
p1
p2
# Test if d' is significantly greater than 0
test_result <- t.test(d_prime_stats$d_prime, mu = 0, alternative = "greater")
print(test_result)
# Test if d' is significantly greater than 0
test_result <- t.test(d_prime_stats$d_prime, mu = 0, alternative = "greater")
print(test_result)
# 1. Calculate Accuracy per Participant
simple_accuracy <- d_prime_after |>
group_by(ID) |>
summarise(
# Check if the outcome was one of the two "Correct" types
n_correct = sum(trial_outcome %in% c("Hit", "Correct Rejection")),
n_total   = n(),
accuracy  = n_correct / n_total
)
library(tidyverse)
library(rstatix) # for ANOVA
library(lme4) # for LMM
library(lmerTest)
forced_choice_path <- "~/Desktop/Python/EM2-audio/Analysis/combined_control_data.csv"
forced_choice_df <- read_delim(forced_choice_path) |>
group_by(`Participant ID`) |>
mutate(trial_i = row_number()) |> ungroup()  |>
rename(ID = `Participant ID`) |>
relocate(ID, .after = trial_i) |>
filter(main.thisN >= 0) |> # remove pratice trial
filter(rt < 2) # must be quicker than 2 sec
d_prime_path <- "~/Desktop/Python/EM2-audio/Analysis/combined_dprime_after_main_experiment.csv"
d_prime_df <- read_delim(d_prime_path) |>
group_by(`Participant ID`) |>
mutate(trial_i = row_number()) |> ungroup()  |>
rename(ID = `Participant ID`) |>
relocate(trial_i, ID, .before = signal_type) |>
filter(main.thisTrialN >= 0) |> # remove pratice trial
filter(rt < 2) # must be quicker than 2 sec
d_prime_before <- d_prime_df |>
filter(!is.na(compression_level.1))
d_prime_after <- d_prime_df |>
filter(is.na(compression_level.1))
participant_acc <- forced_choice_df |>
group_by(ID, prime_valence) |>
summarise(mean_acc = mean(accuracy, na.rm = TRUE), .groups = "drop")
# 2. Calculate Group Averages
group_summary <- participant_acc |>
group_by(prime_valence) |>
summarise(
group_mean = mean(mean_acc),
sd = sd(mean_acc),
n = n(),
se = sd / sqrt(n)
)
print(group_summary)
forced_choice_df |>
# 1. Calculate total accuracy per person (ignoring valence this time)
group_by(ID) |>
summarise(
total_acc = mean(accuracy, na.rm = TRUE),
# Calculate a simple standard error for the error bars
se = sd(accuracy, na.rm = TRUE) / sqrt(n())
) |>
# 2. Reorder ID by Accuracy (lowest to highest)
mutate(ID = fct_reorder(ID, total_acc)) |>
# 3. Plot
ggplot(aes(x = ID, y = total_acc)) +
# Chance line
geom_hline(yintercept = 0.5, color = "red", linetype = "dashed") +
# Error bars (Confidence interval approximation)
geom_errorbar(aes(ymin = total_acc - se, ymax = total_acc + se), width = 0.2) +
# Points
geom_point(size = 3, color = "steelblue") +
# Labels
labs(title = "Individual Visibility Check",
subtitle = "Participants distinct from the red line (0.5) might be aware of the prime.",
y = "Mean Accuracy",
x = "Participant ID") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Rotate IDs to read them
# 1. Calculate Accuracy per Participant
simple_accuracy <- d_prime_after |>
group_by(ID) |>
summarise(
# Check if the outcome was one of the two "Correct" types
n_correct = sum(trial_outcome %in% c("Hit", "Correct Rejection")),
n_total   = n(),
accuracy  = n_correct / n_total
)
# Preview the data
head(simple_accuracy)
# 1. Calculate Signal Detection metrics per participant
d_prime_stats <- d_prime_after |>
group_by(ID) |>
summarise(
# Count outcomes
hits = sum(trial_outcome == "Hit"),
fas  = sum(trial_outcome == "False Alarm"),
n_signal = sum(signal_type == "present"),
n_noise  = sum(signal_type == "absent"),
.groups = "drop"
) |>
mutate(
# Log-linear correction (add 0.5) to handle perfect performance (0 or 1 rates)
hit_rate = (hits + 0.5) / (n_signal + 1),
fa_rate  = (fas + 0.5) / (n_noise + 1),
# Calculate d' (Sensitivity): Ability to distinguish Signal from Noise
# 0 = Guessing, > 0 = Detection
d_prime = qnorm(hit_rate) - qnorm(fa_rate),
# Calculate c (Bias): Tendency to say "Yes" or "No"
# 0 = Neutral, Negative = Trigger Happy (Yes!), Positive = Conservative (No!)
c_bias = -0.5 * (qnorm(hit_rate) + qnorm(fa_rate))
)
# View the first few rows
head(d_prime_stats)
# Create a composite plot
library(patchwork) # If you don't have this, you can run the plots separately
# Plot A: Distribution of Sensitivity (d')
p1 <- d_prime_stats |>
ggplot(aes(x = d_prime)) +
geom_histogram(binwidth = 0.2, fill = "steelblue", color = "white", alpha = 0.8) +
geom_vline(xintercept = 0, linetype = "dashed", color = "red", size = 1) +
labs(title = "Distribution of Sensitivity (d')",
subtitle = "Red Line = Chance Performance (0).",
x = "d' (Sensitivity)",
y = "Count of Participants") +
theme_minimal()
# Plot B: Bias vs Sensitivity
p2 <- d_prime_stats |>
ggplot(aes(x = c_bias, y = d_prime, label = ID)) +
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
geom_vline(xintercept = 0, linetype = "dotted", color = "gray") +
geom_point(size = 3, alpha = 0.6) +
geom_text(vjust = -0.5, size = 3, check_overlap = TRUE) +
labs(title = "Participant Strategy Map",
subtitle = "Are they hearing it (Y-axis) or just guessing (X-axis)?",
x = "Response Bias (c) \n(< 0 = Liberal/Yes-bias, > 0 = Conservative/No-bias)",
y = "Sensitivity (d')") +
theme_minimal()
# Display them
p1
p2
# Test if d' is significantly greater than 0
test_result <- t.test(d_prime_stats$d_prime, mu = 0, alternative = "greater")
print(test_result)
sdt_stats <- d_prime_before |>
group_by(ID, compression_level) |>
summarise(
# Count specific outcomes
hits = sum(trial_outcome == "Hit"),
misses = sum(trial_outcome == "Miss"),
crs = sum(trial_outcome == "Correct Rejection"),
fas = sum(trial_outcome == "False Alarm"),
# Calculate totals
n_signal_trials = hits + misses,
n_noise_trials = crs + fas,
.groups = "drop"
) |>
# 2. Calculate d-prime and Criterion (c) with Log-Linear Correction
mutate(
# Add 0.5 to counts and 1 to denominator to handle 0s and 1s (Hautus, 1995)
hit_rate_adj = (hits + 0.5) / (n_signal_trials + 1),
fa_rate_adj = (fas + 0.5) / (n_noise_trials + 1),
# Calculate d' (Sensitivity)
d_prime = qnorm(hit_rate_adj) - qnorm(fa_rate_adj),
# Calculate c (Response Bias)
# Negative c = Trigger Happy (Says "Present" too much)
# Positive c = Conservative (Says "Absent" too much)
c_bias = -0.5 * (qnorm(hit_rate_adj) + qnorm(fa_rate_adj))
)
# Preview the data
head(sdt_stats)
sdt_stats |>
ggplot(aes(x = factor(compression_level), y = d_prime)) +
# 1. Individual Participant lines (Spaghetti plot)
geom_line(aes(group = ID), color = "gray", alpha = 0.3) +
geom_point(aes(group = ID), color = "gray", alpha = 0.3) +
# 2. Group Average (Thick Red Line)
stat_summary(aes(group = 1), fun = mean, geom = "line", color = "red", size = 1.5) +
stat_summary(aes(group = 1), fun = mean, geom = "point", color = "red", size = 3) +
# 3. Add d' thresholds for context
geom_hline(yintercept = 0, linetype = "dashed") + # Chance
annotate("text", x = 1, y = 0.2, label = "Chance", color = "black", size = 3) +
labs(
title = "Auditory Sensitivity (d') by Compression Level",
subtitle = "Red line = Group Average. Gray lines = Individuals.\nHigher d' means better detection.",
x = "Compression Level (Signal Strength)",
y = "Sensitivity (d')"
) +
theme_minimal()
# Summarize one average value per participant
participant_overall <- sdt_stats |>
group_by(ID) |>
summarise(
avg_d_prime = mean(d_prime),
avg_bias = mean(c_bias)
)
participant_overall |>
ggplot(aes(x = avg_bias, y = avg_d_prime, label = ID)) +
# Quadrant Lines
geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.5) + # No Bias
geom_hline(yintercept = 0, linetype = "dashed", alpha = 0.5) + # Chance performance
# Points
geom_point(size = 3, color = "steelblue") +
# Add labels for outliers
geom_text(vjust = -0.5, size = 3, check_overlap = TRUE) +
labs(
title = "Participant Performance Map",
subtitle = "Top-Center is ideal (High Sensitivity, No Bias).\nRight = Conservative (Misses signals). Left = Liberal (False Alarms).",
x = "Response Bias (c)",
y = "Average Sensitivity (d')"
) +
theme_minimal()
simple_accuracy <- d_prime_after |>
group_by(ID) |>
summarise(
# Check if the outcome was one of the two "Correct" types
n_correct = sum(trial_outcome %in% c("Hit", "Correct Rejection")),
n_total   = n(),
accuracy  = n_correct / n_total
)
# Preview the data
head(simple_accuracy)
simple_accuracy |>
ggplot(aes(x = "Participants", y = accuracy)) +
# 1. The Chance Line (50%)
geom_hline(yintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
# 2. Boxplot to show the group average and spread
geom_boxplot(fill = "white", color = "black", width = 0.3, outlier.shape = NA) +
# 3. Individual Points (Jittered)
geom_jitter(width = 0.1, size = 3, alpha = 0.6, color = "steelblue") +
# 4. Labels
labs(title = "Detection Task Accuracy",
subtitle = "Red Line = 50% (Random Guessing). Points above are performing better than chance.",
y = "Percent Correct (Hits + Correct Rejections)",
x = "") +
scale_y_continuous(limits = c(0, 1), labels = scales::percent) + # Show Y axis as 0% - 100%
theme_minimal()
# One-sample t-test against 0.5
stat_test <- simple_accuracy |>
t_test(accuracy ~ 1, mu = 0.5, alternative = "greater")
stat_test
simple_accuracy |>
ggplot(aes(x = "Participants", y = accuracy)) +
# 1. The Chance Line (50%)
geom_hline(yintercept = 0.5, linetype = "dashed", color = "red", size = 1) +
# 2. Boxplot to show the group average and spread
geom_boxplot(fill = "white", color = "black", width = 0.3, outlier.shape = NA) +
# 3. Individual Points (Jittered)
geom_jitter(width = 0.1, size = 3, alpha = 0.6, color = "steelblue") +
# 4. Labels
labs(title = "Detection Task Accuracy",
subtitle = "Red Line = 50% (Random Guessing). Points above are performing better than chance.",
y = "Percent Correct (Hits + Correct Rejections)",
x = "") +
scale_y_continuous(limits = c(0, 1), labels = scales::percent) + # Show Y axis as 0% - 100%
theme_minimal()
